java web crawl
B/S 和C/S模式   browser/server     client/server



2. robots  爬虫相关协议
rotbos.txt写了哪些信息允许爬取,放在根目录下，每行第一个字母要大写。还有User-agent属性，allow与disallow定义希望爬取的内容。
例子
User-agent: Baiduspider
Allow: /article
Allow: /oshtml
...
Disallow:/ product
...

3.爬虫入门
使用$(.class_name).text()获取网页内容
code in eclipse


4.爬虫策略详解
深度优先爬取，  有可能死循环，因为非常深。
宽度优先爬取，
非完全PageRank,  以网页依赖程度排序
大站优先爬取策略，以网站被下载次数排序


5.爬虫流程
/爬取对象准备，页面数据抓取，数据解析，数据存储/
URL去重使用redis服务器去重。
网页数据抓取，通过Jsoup HttpClient,存储的话使用hadoop分布式存储。
反爬虫措施：用户代理限制（cookie），用户名密码，验证码，IP限制(动态IP)，。。。
数据解析：常用的解析工具：Xpath，Jsoup选择器解析,HtmlUtil动态解析(主要用来解析JS或者AJAX 动态获取信息),Selenium自动化工具
数据存储：数据去噪(存储需要存储的)，数据去重（使用simhahs算法），数据存储到(nosql(redis,mongodb,hbase),mysql,elasticsearch)


6.Xpath的使用(解析XML，解析HTML)
简介，语法，解析XML，解析HTML
Xpath就是XML路径语言，用来确定XML某个节点的路径，也能解析HTML文档。
基于XML的树状结构！！！ HTML也是树状模型！！！（DOM）
使用路径表达式在XML文档中导航，包含一个标准函数库。

Xpath的语法
nodename:选取此节点的所有子节点。
/：从根节点选取
// :从匹配选择的当前节点选择文档中的节点，不考虑它们的位置。
.   当前节点
..  当前节点的父节点
@   选取属性
* 匹配任何元素节点
@* 匹配任何属性节点
node() 匹配任何类型的节点
例子

<?xml version="1.0" encoding="UTF-8"?>
<bookstore>
	<book>
		<title lang = "eng">Harry Potter</title>
		<price>29.99</price>
	</book>
	<book>
		<title lang="eng">learning XML</title>
		<price>39.95</price>
	</book>
	<info>
		<book>
			<title lang="eng">learning XMLLLL</title>
			<price>39</price>
		</book>
	</info>
</bookstore>

bookstore//book  选择属于bookstore元素的后代的所有book元素，而不管它们处于bookstore下面的什么位置
//@lang 选取名字为lang的所有属性
/bookstore/book[1]  选取bookstore子元素的第一个book元素
/bookstore/book[last()]  选取bookstore子元素的最后一个book元素
/bookstore/book[last() - 1]   倒数第二个
/bookstore/book[position() < 3] 前两个
//title[@lang] 选取所有拥有名字为lang属性的title元素
//title[@lang='end'] 选取所有title元素，且这些元素值为eng
/bookstore/book[price>35.00] 选取bookstore元素的所有book元素，且其中的price元素值大于35.00
/bookstore/*   bookstore元素的所有子元素
//*  文档中所有元素
//title[@*]  所有带有title的元素
----------------------------------------------------
XML解析实例
借助dom4j和jaxen工具包
//HTML 解析实例
工具Xpath-helper插件安装
ctrl+shift+x打开Xpath-helper，鼠标右键可以得到xpath的表达式。






7.Jsoup解析工具
基于Java的HTML解析器，具有省力的API，通过DOM,CSS及类似JQuery的操作取出标签属性。。
准备爬取URL=》创建连接获取URL页面信息=》页面信息转出DOM对象=》通过Jsoup标签解析器获取页面元素


8.jsoup的selector选择器
jsoup的常用API:
Connection connect创建URL连接
Document parse(file in, String charsetName)  以指定字符集解析HTML文件为DOM文档
Document parse(String html) 把HTML字符串转换为DOM文档
String clean(String bodyHtml, WhiteList whitelist) 使用whitelist对输入的html文档过滤，只允许特定的标签或者属性，防止恶意代码。


Document类方法：
Element body() 得到Document body部分的信息
String title()  得到Document的title
getElementById()  通过Id获取元素
getElementByTag() 
getElementByAttribute()
doc.select(cssQuery)
doc.selectFirst()
text() 获取文本信息


Element类方法：
getElementById/Tag/Attribute
select()
text()
attr(String key) 根据属性值获取元素的文本信息

例子：Element element = doc.getElementById("ataginfo")
String value = element.attr("href");
value结果就是www.baidu.com
Jsoup.connect(href);
继续抓取百度。。。



Jsoup选择器
tagname,
#id,
.class,
[attribute],
[attr=value],
*
还可以使用组合选择器

伪选择器selectors。。。

在web page inspect copy selector得到选择器

